{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0+XhbS33eff+xX8tRVmER"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Syntax\n"
      ],
      "metadata": {
        "id": "fm7REZtCeoRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About"
      ],
      "metadata": {
        "id": "ook83oYjfZq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract data from Kampus Merdeka, Tokopedia, News Headlines, E-Commerce, and Playstore Reviews. Analyze trends with Python scripts. Demo: [Hugging Face](https://huggingface.co/naufalnashif).\n",
        "\n",
        "#### Features\n",
        "- **Kampus Merdeka Scraper:** Extracts courses and schedules.\n",
        "- **Tokopedia Scraper:** Gathers product details and reviews.\n",
        "- **News Headline Scraper:** Retrieves latest news headlines.\n",
        "- **E-Commerce Scraper:** Extracts product details and prices.\n",
        "- **Playstore Reviews Scraper:** Captures user feedback.\n",
        "\n",
        "#### Usage\n",
        "Use Python scripts for each platform. Adapt and integrate for your projects.\n",
        "\n",
        "#### Application Demo\n",
        "Check the live demo on [Hugging Face](https://huggingface.co/naufalnashif) for hands-on experience.\n",
        "\n",
        "**Note:** Respect website policies and legal considerations.\n"
      ],
      "metadata": {
        "id": "_vvdnH1BfIdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Kampus Merdeka"
      ],
      "metadata": {
        "id": "CzaPVnnqRmvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "JBb25zPSRr0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ],
      "metadata": {
        "id": "MAEk7Rg6RzOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Dependencies"
      ],
      "metadata": {
        "id": "vquIjQhxRu1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "fWO-CQgjRxj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIEOi-1DQndz"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    url = 'https://api.kampusmerdeka.kemdikbud.go.id/magang/browse/opportunities?opportunity_type=MSIB&activity_type=&offset=25&limit=8000'\n",
        "    user_agents = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.2 Safari/605.1.15\",\n",
        "    ]\n",
        "    # Mendapatkan User-Agent acak\n",
        "    random_user_agent = random.choice(user_agents)\n",
        "    # Menggunakan User-Agent dalam permintaan HTTP\n",
        "    headers = {\n",
        "        \"User-Agent\": random_user_agent,\n",
        "        \"Accept-Language\": \"en-US,en;q=0.5\"\n",
        "    }\n",
        "    timeout = 10\n",
        "    response = requests.get(url, headers=headers, timeout=timeout)\n",
        "    df = pd.DataFrame(response.json()['data'])\n",
        "    print(df.info())\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scraping Tokopedia"
      ],
      "metadata": {
        "id": "EsngbLK9Sb-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import quote\n",
        "\n",
        "def scrape_tokped(nama_barang, num_items):\n",
        "    products = []\n",
        "    page = 1\n",
        "    query = quote(nama_barang)\n",
        "    while len(products) < num_items :\n",
        "        url = f'https://www.tokopedia.com/search?navsource=&page={page}&q={query}&srp_component_id=02.01.00.00&srp_page_id=&srp_page_title=&st='\n",
        "\n",
        "        user_agents = [\n",
        "          \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
        "          \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134\",\n",
        "          \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.2 Safari/605.1.15\",\n",
        "        ]\n",
        "        # Mendapatkan User-Agent acak\n",
        "        random_user_agent = random.choice(user_agents)\n",
        "        # Menggunakan User-Agent dalam permintaan HTTP\n",
        "        headers = {\n",
        "            \"User-Agent\": random_user_agent,\n",
        "            \"Accept-Language\": \"en-US,en;q=0.5\"\n",
        "        }\n",
        "        timeout = 10\n",
        "        try :\n",
        "            response = requests.get(url, headers = headers, timeout = timeout)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            product_container_list = soup.find_all('a', class_=\"pcv3__info-content css-gwkf0u\", href = True)\n",
        "\n",
        "            for product_info in product_container_list:\n",
        "                link = product_info['href']\n",
        "                title_element = product_info.find('div', class_=\"prd_link-product-name css-3um8ox\")\n",
        "                title = title_element.text.strip() if title_element else None\n",
        "\n",
        "                harga_element = product_info.find('div', class_=\"prd_link-product-price css-h66vau\")\n",
        "                harga = harga_element.text.strip() if harga_element else None\n",
        "\n",
        "                terjual_element = product_info.find('span', class_=\"prd_label-integrity css-1sgek4h\")\n",
        "                terjual = terjual_element.text if terjual_element else None\n",
        "\n",
        "                rating_element = product_info.find('span', class_='prd_rating-average-text css-t70v7i')\n",
        "                rating = rating_element.text if rating_element else None\n",
        "\n",
        "                toko_element = product_info.find('span', class_=\"prd_link-shop-name css-1kdc32b flip\")\n",
        "                toko = toko_element.text.strip() if toko_element else None\n",
        "\n",
        "                asal_product_element = product_info.find('span', class_=\"prd_link-shop-loc css-1kdc32b flip\")\n",
        "                asal_product = asal_product_element.text.strip() if asal_product_element else None\n",
        "\n",
        "                products.append({\n",
        "                    'link': link,\n",
        "                    'produk' : title,\n",
        "                    'harga' : harga,\n",
        "                    'terjual' : terjual,\n",
        "                    'rating' : rating,\n",
        "                    'toko' : toko,\n",
        "                    'asal_product' : asal_product,\n",
        "                })\n",
        "            if len(products) >= num_items:\n",
        "                products = products[:num_items]\n",
        "                break\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "        page += 1\n",
        "    return products"
      ],
      "metadata": {
        "id": "UZJpW-EDSfQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nama_barang = 'Iphone 12 Pro'\n",
        "num_items = 10000\n",
        "hasil = scrape_tokped(nama_barang, num_items)\n",
        "\n",
        "pd.DataFrame(hasil)"
      ],
      "metadata": {
        "id": "6ufEjZJeSlhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Playstore Review (With Streamlit Framework)\n",
        "Aplication demo on :https://huggingface.co/spaces/naufalnashif/scraping-playstore-reviews/"
      ],
      "metadata": {
        "id": "uZS3KycfSn-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies"
      ],
      "metadata": {
        "id": "iruc9bCLTYim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit google-play-scraper"
      ],
      "metadata": {
        "id": "aKYMcHX0TbDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "#from google_play_scraper import app, Sort, reviews, permission, reviews_all, search\n",
        "from google_play_scraper import app, Sort, reviews, reviews_all, permissions, search\n",
        "import re\n",
        "\n",
        "#---------------------------------------------func----------------------------------\n",
        "\n",
        "@st.cache_data\n",
        "def get_url_by_app_name(nama_apl):\n",
        "    \"\"\"\n",
        "    Mengembalikan URL aplikasi berdasarkan nama aplikasi dari kamus.\n",
        "    Parameters:\n",
        "    - nama_apl (str): Nama aplikasi yang dicari.\n",
        "    - aplikasi_dict (dict): Kamus yang memetakan nama aplikasi ke URL.\n",
        "    Returns:\n",
        "    - str or None: URL aplikasi atau None jika tidak ditemukan.\n",
        "    \"\"\"\n",
        "    list_url = [\n",
        "        'https://play.google.com/store/apps/details?id=com.shopee.id',\n",
        "        'https://play.google.com/store/apps/details?id=com.tokopedia.tkpd',\n",
        "        'https://play.google.com/store/apps/details?id=com.amazon.mShop.android.shopping',\n",
        "        'https://play.google.com/store/apps/details?id=com.grabtaxi.passenger'\n",
        "    ]\n",
        "    aplikasi_dict = {\n",
        "        'Shopee': list_url[0],\n",
        "        'Tokopedia': list_url[1],\n",
        "        'Amazon': list_url[2],\n",
        "        'Grab': list_url[3]\n",
        "    }\n",
        "    return aplikasi_dict.get(nama_apl, None)\n",
        "\n",
        "@st.cache_data\n",
        "def extract_app_id(play_store_url):\n",
        "    # Definisikan pola ekspresi reguler untuk menemukan ID aplikasi\n",
        "    pattern = r'id=([a-zA-Z0-9._]+)'\n",
        "\n",
        "    # Gunakan ekspresi reguler untuk mencocokkan pola dalam URL\n",
        "    match = re.search(pattern, play_store_url)\n",
        "\n",
        "    # Periksa apakah ada kecocokan dan kembalikan ID aplikasi jika ada\n",
        "    if match:\n",
        "        app_id = match.group(1)\n",
        "        return app_id\n",
        "    else:\n",
        "        return None\n",
        "@st.cache_data\n",
        "def scraping_func(app_id, bahasa, negara, filter_score, jumlah):\n",
        "    filter_score = None if filter_score == \"Semua Rating\" else filter_score\n",
        "\n",
        "    rws, token = reviews(\n",
        "        app_id,\n",
        "        lang=bahasa,\n",
        "        country=negara,\n",
        "        sort=Sort.NEWEST,\n",
        "        filter_score_with=filter_score,\n",
        "        count=jumlah\n",
        "    )\n",
        "\n",
        "    scraping_done = bool(rws)\n",
        "\n",
        "    return rws, token, scraping_done\n",
        "\n",
        "@st.cache_data\n",
        "def scraping_all_func(app_id, bahasa, negara, filter_score, sleep = 0):\n",
        "    filter_score = None if filter_score == \"Semua Rating\" else filter_score\n",
        "\n",
        "    rws = reviews_all(\n",
        "        app_id,\n",
        "        sleep_milliseconds=sleep, # defaults to 0\n",
        "        lang=bahasa,\n",
        "        country=negara,\n",
        "        filter_score_with=filter_score,\n",
        "    )\n",
        "\n",
        "    scraping_done = bool(rws)\n",
        "\n",
        "    return rws, scraping_done\n",
        "\n",
        "@st.cache_data\n",
        "def buat_chart(df, target_year):\n",
        "    st.write(f\"Bar Chart Tahun {target_year}:\")\n",
        "\n",
        "    # Ambil bulan\n",
        "    df['at'] = pd.to_datetime(df['at'])  # Convert 'at' column to datetime\n",
        "    df['month'] = df['at'].dt.month\n",
        "    df['year'] = df['at'].dt.year\n",
        "\n",
        "    # Filter DataFrame for the desired year\n",
        "    df_filtered = df[df['year'] == target_year]\n",
        "\n",
        "    # Check if data for the target year is available\n",
        "    if df_filtered.empty:\n",
        "        st.warning(f\"Tidak ada data untuk tahun {target_year}.\")\n",
        "        return\n",
        "\n",
        "    # Mapping nilai bulan ke nama bulan\n",
        "    bulan_mapping = {\n",
        "        1: f'Januari {target_year}',\n",
        "        2: f'Februari {target_year}',\n",
        "        3: f'Maret {target_year}',\n",
        "        4: f'April {target_year}',\n",
        "        5: f'Mei {target_year}',\n",
        "        6: f'Juni {target_year}',\n",
        "        7: f'Juli {target_year}',\n",
        "        8: f'Agustus {target_year}',\n",
        "        9: f'September {target_year}',\n",
        "        10: f'Oktober {target_year}',\n",
        "        11: f'November {target_year}',\n",
        "        12: f'Desember {target_year}'\n",
        "    }\n",
        "\n",
        "    # Mengganti nilai dalam kolom 'month' menggunakan mapping\n",
        "    df_filtered['month'] = df_filtered['month'].replace(bulan_mapping)\n",
        "\n",
        "    # Menentukan warna untuk setiap kategori dalam kolom 'score'\n",
        "    warna_score = {\n",
        "        1: '#FF9AA2',\n",
        "        2: '#FFB7B2',\n",
        "        3: '#FFDAC1',\n",
        "        4: '#E2F0CB',\n",
        "        5: '#B5EAD7'\n",
        "    }\n",
        "\n",
        "    # Sorting unique scores\n",
        "    unique_scores = sorted(df_filtered['score'].unique())\n",
        "\n",
        "    # Ensure months are in the correct order\n",
        "    months_order = [\n",
        "        f'Januari {target_year}', f'Februari {target_year}', f'Maret {target_year}', f'April {target_year}', f'Mei {target_year}', f'Juni {target_year}',\n",
        "        f'Juli {target_year}', f'Agustus {target_year}', f'September {target_year}', f'Oktober {target_year}', f'November {target_year}', f'Desember {target_year}'\n",
        "    ]\n",
        "\n",
        "    # Sort DataFrame based on the custom order of months\n",
        "    df_filtered['month'] = pd.Categorical(df_filtered['month'], categories=months_order, ordered=True)\n",
        "    df_filtered = df_filtered.sort_values('month')\n",
        "\n",
        "    # Create a bar chart with stacking and manual colors\n",
        "    st.bar_chart(\n",
        "        df_filtered.groupby(['month', 'score']).size().unstack().fillna(0),\n",
        "        color=[warna_score[score] for score in unique_scores]\n",
        "    )\n",
        "#--------------------------------------------UI---------------------------------------\n",
        "# Streamlit UI\n",
        "st.title(\"Data Everywhere : Scraping Playstore Reviews\")\n",
        "with st.expander(\"Scraping Settings :\"):\n",
        "    scrape = st.selectbox(\"PIlih Metode :\", (\"Semua Reviews\", \"Estimasi Data\"), index = 1)\n",
        "    aplikasi = st.radio(\n",
        "        \"Pilih Input :\",\n",
        "        [\"Defaults\", \"Custom URL\"], index = 0,\n",
        "        captions = [\"Shopee, Tokopedia, Amazon, Grab\", \"Tambahkan URL Manual\"])\n",
        "    if aplikasi == \"Defaults\" :\n",
        "        nama_apl = st.selectbox(\"Pilih Aplikasi :\", ('Shopee', 'Tokopedia', 'Amazon', 'Grab'))\n",
        "        if nama_apl :\n",
        "            url = get_url_by_app_name(nama_apl)\n",
        "    elif aplikasi == \"Custom URL\":\n",
        "        url = st.text_input(\"Masukkan URL Aplikasi Pada Web Playstore :\", 'https://play.google.com/store/apps/details?id=com.shopee.id')\n",
        "    if scrape == \"Estimasi Data\" :\n",
        "        jumlah = st.number_input(\"Masukkan Estimasi Banyak Data :\", min_value = 10, max_value = 10000, step = 10, placeholder=\"Type a number...\")\n",
        "with st.expander(\"Preference Settings :\"):\n",
        "    if scrape == \"Semua Reviews\" :\n",
        "        sleep = st.number_input(\"Masukkan sleep (milisecond) :\", min_value = 1, max_value = 1000, step = 10, placeholder=\"Type a number...\")\n",
        "    bahasa = st.selectbox(\"Pilih Bahasa:\", ('en', 'id'))\n",
        "    negara = st.selectbox(\"Pilih Negara :\", ('us', 'id'))\n",
        "    filter_score = st.selectbox(\"Pilih Rating :\", ('Semua Rating', 1, 2, 3, 4, 5))\n",
        "    target_year = st.selectbox(\"Pilih Tahun Bar Chart :\", (2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025), index = 4)\n",
        "    download_format = st.selectbox(\"Pilih Format Unduhan :\", [\"XLSX\", \"CSV\", \"JSON\"])\n",
        "st.info('Tekan \"Mulai Scraping\" kembali jika tampilan menghilang ', icon=\"ℹ️\")\n",
        "#-------------------------------------------BE----------------------------------------\n",
        "\n",
        "scraping_done = False\n",
        "if url and bahasa and negara and filter_score and download_format:\n",
        "    if st.button (\"Mulai Scraping\") :\n",
        "        app_id = extract_app_id(url)\n",
        "        if scrape == \"Semua Reviews\" :\n",
        "            reviews, scraping_done = scraping_all_func(app_id, bahasa, negara, filter_score, sleep)\n",
        "            df = pd.DataFrame(reviews)\n",
        "        elif scrape == \"Estimasi Data\":\n",
        "            reviews, token, scraping_done = scraping_func(app_id, bahasa, negara, filter_score, jumlah)\n",
        "            df = pd.DataFrame(reviews)\n",
        "        else :\n",
        "            st.warning(\"Masukkan pilihan yang valid\")\n",
        "else :\n",
        "    st.error(\"Mohon Masukkan Parameter.\")\n",
        "\n",
        "if scraping_done == True:\n",
        "    with st.expander(f\"Hasil Scraping {app_id}:\"):\n",
        "        st.write(df)\n",
        "\n",
        "    buat_chart(df, target_year)\n",
        "\n",
        "    if download_format == \"XLSX\":\n",
        "        # Clean the data to remove illegal characters\n",
        "        cleaned_data = df.applymap(lambda x: \"\".join(char for char in str(x) if char.isprintable()))\n",
        "\n",
        "        # Save the cleaned data to Excel\n",
        "        cleaned_data.to_excel(f\"hasil_scraping_{app_id}.xlsx\", index=False)\n",
        "\n",
        "        # Provide the download button for the cleaned Excel file\n",
        "        st.download_button(label=f\"Unduh XLSX ({len(reviews)} data)\", data=open(f\"hasil_scraping_{app_id}.xlsx\", \"rb\").read(), key=\"xlsx_download\", file_name=f\"hasil_scraping_{app_id}.xlsx\")\n",
        "\n",
        "    elif download_format == \"CSV\":\n",
        "        csv = df.to_csv(index=False)\n",
        "\n",
        "        # Provide the download button for the CSV file\n",
        "        st.download_button(label=f\"Unduh CSV ({len(reviews)} data)\", data=csv, key=\"csv_download\", file_name=f\"hasil_scraping_{app_id}.csv\")\n",
        "\n",
        "    elif download_format == \"JSON\":\n",
        "        json_data = df.to_json(orient=\"records\")\n",
        "\n",
        "        # Provide the download button for the JSON file\n",
        "        st.download_button(label=f\"Unduh JSON ({len(reviews)} data)\", data=json_data, key=\"json_download\", file_name=f\"hasil_scraping_{app_id}.json\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Tidak ada data\")\n",
        "\n",
        "st.divider()\n",
        "github_link = \"https://github.com/naufalnashif/\"\n",
        "st.markdown(f\"GitHub: [{github_link}]({github_link})\")\n",
        "instagram_link = \"https://www.instagram.com/naufal.nashif/\"\n",
        "st.markdown(f\"Instagram: [{instagram_link}]({instagram_link})\")\n",
        "st.write('Terima kasih telah mencoba demo ini!')"
      ],
      "metadata": {
        "id": "-CgzMQZuTJ7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping E-Commerce with Streamlit Framework (Klikindomaret, Tokopedia, Tokopedia(Selenium))\n",
        "Aplication Demo on : https://huggingface.co/spaces/naufalnashif/scraping-ecommerce-2023/"
      ],
      "metadata": {
        "id": "p_C0-on-UFQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies"
      ],
      "metadata": {
        "id": "wIgIJjP_WpgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pandas numpy requests beautifulsoup4 matplotlib seaborn wordcloud datetime collections nltk urllib3 requests-html selenium"
      ],
      "metadata": {
        "id": "ZPGz51BzWrsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Syntax"
      ],
      "metadata": {
        "id": "IG-9nIaIWv7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import quote\n",
        "import streamlit as st\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "from requests_html import HTMLSession\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "@st.cache_data\n",
        "def scrape_klikindomaret(nama_barang, num_items):\n",
        "    products = []\n",
        "    page = 1\n",
        "    query = quote(nama_barang)\n",
        "\n",
        "\n",
        "    while len(products) < num_items :\n",
        "        if len (products) > num_items :\n",
        "            products = products[:num_items]\n",
        "            break\n",
        "        url = f\"https://www.klikindomaret.com/search/?key={query}&categories=&productbrandid=&sortcol=&pagesize=54&page={page}&startprice=&endprice=&attributes=&ShowItem=\"\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        product_list = soup.find_all('a', href=True)\n",
        "\n",
        "        for product in product_list:\n",
        "\n",
        "            product_href = product['href']\n",
        "            if '/product/' in product_href:\n",
        "                product_name = product.find('div', class_='title').text.strip()\n",
        "                product_price = product.find('span', class_='normal price-value').text.strip()\n",
        "\n",
        "                 # Cek apakah ada harga sebelum diskon dan persentase diskon\n",
        "                discount_element = product.find('span', class_='strikeout disc-price')\n",
        "                discount_percentage = \"\"\n",
        "                original_price = \"\"\n",
        "                if discount_element:\n",
        "                    discount_percentage = discount_element.find('span', class_='discount').text.strip()\n",
        "                    original_price = discount_element.text.replace(discount_percentage, '').strip()\n",
        "                else:\n",
        "                    # Jika tidak ada diskon, set discount_percentage ke \"0%\" dan original_price ke product_price\n",
        "                    discount_percentage = \"0%\"\n",
        "                    original_price = product_price\n",
        "\n",
        "                product_link = f\"https://www.klikindomaret.com{product_href}\"\n",
        "                products.append({\n",
        "                    'product': product_name,\n",
        "                    'original_price': original_price,\n",
        "                    'discount_percentage': discount_percentage,\n",
        "                    'price': product_price,\n",
        "                    'link': product_link\n",
        "                })\n",
        "            if len (products) > num_items :\n",
        "                products = products[:num_items]\n",
        "                break\n",
        "        page += 1\n",
        "\n",
        "    return products\n",
        "\n",
        "@st.cache_data\n",
        "def scrape_tokped(nama_barang, num_items):\n",
        "    products = []\n",
        "    page = 1\n",
        "    query = quote(nama_barang)\n",
        "    while len(products) < num_items :\n",
        "        url = f'https://www.tokopedia.com/search?navsource=&page={page}&q={query}&srp_component_id=02.01.00.00&srp_page_id=&srp_page_title=&st='\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'Accept-Language': 'en-US,en;q=0.9,id-ID;q=0.8,id;q=0.7,ja;q=0.6,ru;q=0.5,zh-CN;q=0.4,zh;q=0.3,af;q=0.2,nl;q=0.1',\n",
        "            'Cache-Control': 'max-age=0',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        }\n",
        "        timeout = 10\n",
        "        try :\n",
        "            response = requests.get(url, headers = headers, timeout = timeout)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            product_container_list = soup.find_all('a', class_=\"pcv3__info-content css-gwkf0u\", href = True)\n",
        "\n",
        "            for product_info in product_container_list:\n",
        "                link = product_info['href']\n",
        "                title_element = product_info.find('div', class_=\"prd_link-product-name css-3um8ox\")\n",
        "                title = title_element.text.strip() if title_element else None\n",
        "\n",
        "                harga_element = product_info.find('div', class_=\"prd_link-product-price css-h66vau\")\n",
        "                harga = harga_element.text.strip() if harga_element else None\n",
        "\n",
        "                terjual_element = product_info.find('span', class_=\"prd_label-integrity css-1sgek4h\")\n",
        "                terjual = terjual_element.text if terjual_element else None\n",
        "\n",
        "                rating_element = product_info.find('span', class_='prd_rating-average-text css-t70v7i')\n",
        "                rating = rating_element.text if rating_element else None\n",
        "\n",
        "                toko_element = product_info.find('span', class_=\"prd_link-shop-name css-1kdc32b flip\")\n",
        "                toko = toko_element.text.strip() if toko_element else None\n",
        "\n",
        "                asal_product_element = product_info.find('span', class_=\"prd_link-shop-loc css-1kdc32b flip\")\n",
        "                asal_product = asal_product_element.text.strip() if asal_product_element else None\n",
        "\n",
        "                products.append({\n",
        "                    'link': link,\n",
        "                    'produk' : title,\n",
        "                    'harga' : harga,\n",
        "                    'terjual' : terjual,\n",
        "                    'rating' : rating,\n",
        "                    'toko' : toko,\n",
        "                    'asal_product' : asal_product,\n",
        "                })\n",
        "            if len(products) >= num_items:\n",
        "                products = products[:num_items]\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Terjadi kesalahan yang tidak diketahui: {e}\")\n",
        "            st.write(\"Jalankan script ini di IDE/colab.research.google.com Anda :\")\n",
        "            code =  '''\n",
        "                    !pip install beautifulsoup4\n",
        "                    !pip install requests\n",
        "                    !pip install streamlit\n",
        "                    from bs4 import BeautifulSoup\n",
        "                    import requests\n",
        "                    from urllib.parse import quote\n",
        "                    import pandas as pd\n",
        "                    import streamlit as st\n",
        "                    def scrape_tokped(nama_barang, num_items):\n",
        "                        products = []\n",
        "                        page = 1\n",
        "                        query = quote(nama_barang)\n",
        "                        while len(products) < num_items :\n",
        "                            url = f'https://www.tokopedia.com/search?navsource=&page={page}&q={query}&srp_component_id=02.01.00.00&srp_page_id=&srp_page_title=&st='\n",
        "\n",
        "                            headers = {\n",
        "                                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "                            }\n",
        "                            timeout = 10\n",
        "                            try :\n",
        "                                response = requests.get(url, headers = headers, timeout = timeout)\n",
        "                                response.raise_for_status()\n",
        "\n",
        "                                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                                product_container_list = soup.find_all('a', class_=\"pcv3__info-content css-gwkf0u\", href = True)\n",
        "\n",
        "                                for product_info in product_container_list:\n",
        "                                    link = product_info['href']\n",
        "                                    title_element = product_info.find('div', class_=\"prd_link-product-name css-3um8ox\")\n",
        "                                    title = title_element.text.strip() if title_element else None\n",
        "\n",
        "                                    harga_element = product_info.find('div', class_=\"prd_link-product-price css-h66vau\")\n",
        "                                    harga = harga_element.text.strip() if harga_element else None\n",
        "\n",
        "                                    terjual_element = product_info.find('span', class_=\"prd_label-integrity css-1sgek4h\")\n",
        "                                    terjual = terjual_element.text if terjual_element else None\n",
        "\n",
        "                                    rating_element = product_info.find('span', class_='prd_rating-average-text css-t70v7i')\n",
        "                                    rating = rating_element.text if rating_element else None\n",
        "\n",
        "                                    toko_element = product_info.find('span', class_=\"prd_link-shop-name css-1kdc32b flip\")\n",
        "                                    toko = toko_element.text.strip() if toko_element else None\n",
        "\n",
        "                                    asal_product_element = product_info.find('span', class_=\"prd_link-shop-loc css-1kdc32b flip\")\n",
        "                                    asal_product = asal_product_element.text.strip() if asal_product_element else None\n",
        "\n",
        "                                    products.append({\n",
        "                                        'link': link,\n",
        "                                        'produk' : title,\n",
        "                                        'harga' : harga,\n",
        "                                        'terjual' : terjual,\n",
        "                                        'rating' : rating,\n",
        "                                        'toko' : toko,\n",
        "                                        'asal_product' : asal_product,\n",
        "                                    })\n",
        "                                if len(products) >= num_items:\n",
        "                                    products = products[:num_items]\n",
        "                                    break\n",
        "\n",
        "                            except requests.exceptions.RequestException as e:\n",
        "                                logging.error(f\"Terjadi kesalahan saat mengirim permintaan: {e}\")\n",
        "                                break\n",
        "                            except requests.exceptions.HTTPError as e:\n",
        "                                logging.error(f\"HTTP Error: {e}\")\n",
        "                                break\n",
        "                            except Exception as e:\n",
        "                                logging.error(f\"Terjadi kesalahan yang tidak diketahui: {e}\")\n",
        "                                break\n",
        "                            page += 1\n",
        "                        return products)\n",
        "\n",
        "                    nama_barang = input(\"Masukkan nama barang: \")\n",
        "                    num_items = int(input(\"Masukkan jumlah barang yang ingin diambil: \"))\n",
        "                    # Melakukan scraping menggunakan fungsi scrape_tokped\n",
        "                    hasil = scrape_tokped(nama_barang, num_items)\n",
        "                    pd.DataFrame(hasil)'''\n",
        "            st.code(code, language='python')\n",
        "            break\n",
        "        page += 1\n",
        "    return products\n",
        "\n",
        "@st.cache_data\n",
        "def scrape_tokped_with_selenium(nama_barang, num_items):\n",
        "    products = []\n",
        "    page = 1\n",
        "    query = quote(nama_barang)\n",
        "\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--disable-notifications')\n",
        "    options.add_argument('--disable-infobars')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    while len(products) < num_items :\n",
        "        try :\n",
        "            url = f'https://www.tokopedia.com/search?navsource=&page={page}&q={query}&srp_component_id=02.01.00.00&srp_page_id=&srp_page_title=&st='\n",
        "\n",
        "            driver.get(url)\n",
        "            # Eksekusi JavaScript untuk mengatur header\n",
        "            #driver.execute_script(\n",
        "                #\"\"\"\n",
        "                #var xhr = new XMLHttpRequest();\n",
        "                #xhr.open('GET', arguments[0], false);\n",
        "                #xhr.setRequestHeader('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36');\n",
        "                #xhr.send(null);\n",
        "                #\"\"\"\n",
        "                #, url\n",
        "            #)\n",
        "\n",
        "            # Dapatkan sumber halaman setelah eksekusi JavaScript\n",
        "            # Tunggu hingga halaman selesai dimuat (opsional, tergantung kebutuhan)\n",
        "            driver.implicitly_wait(20)  # Tunggu maksimal 20 detik\n",
        "\n",
        "            # Temukan elemen kontainer produk berdasarkan XPath atau CSS selector\n",
        "            # Di sini, saya menggunakan XPath sebagai contoh:\n",
        "            product_container_xpath = \"//body//*\"  # Ganti dengan XPath yang sesuai\n",
        "            #html = driver.find_elements_by_xpath('//body//*')\n",
        "            #html = driver.find_element(By.XPATH, product_container_xpath)\n",
        "            html = driver.execute_script(\"return document.getElementsByTagName('html')[0].innerHTML\")\n",
        "            st.write(html)\n",
        "            # Gunakan BeautifulSoup untuk melakukan parsing HTML\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "            # Cari semua elemen yang sesuai\n",
        "            product_container_list = soup.find_all('a', class_=\"pcv3__info-content css-gwkf0u\", href=True)\n",
        "\n",
        "            for product_info in product_container_list:\n",
        "                link = product_info['href']\n",
        "                st.write(link)\n",
        "                title_element = product_info.find('div', class_=\"prd_link-product-name css-3um8ox\")\n",
        "                title = title_element.text.strip() if title_element else None\n",
        "\n",
        "                harga_element = product_info.find('div', class_=\"prd_link-product-price css-h66vau\")\n",
        "                harga = harga_element.text.strip() if harga_element else None\n",
        "\n",
        "                terjual_element = product_info.find('span', class_=\"prd_label-integrity css-1sgek4h\")\n",
        "                terjual = terjual_element.text if terjual_element else None\n",
        "\n",
        "                rating_element = product_info.find('span', class_='prd_rating-average-text css-t70v7i')\n",
        "                rating = rating_element.text if rating_element else None\n",
        "\n",
        "                toko_element = product_info.find('span', class_=\"prd_link-shop-name css-1kdc32b flip\")\n",
        "                toko = toko_element.text.strip() if toko_element else None\n",
        "\n",
        "                asal_product_element = product_info.find('span', class_=\"prd_link-shop-loc css-1kdc32b flip\")\n",
        "                asal_product = asal_product_element.text.strip() if asal_product_element else None\n",
        "\n",
        "                products.append({\n",
        "                    'link': link,\n",
        "                    'produk' : title,\n",
        "                    'harga' : harga,\n",
        "                    'terjual' : terjual,\n",
        "                    'rating' : rating,\n",
        "                    'toko' : toko,\n",
        "                    'asal_product' : asal_product,\n",
        "                })\n",
        "            if len(products) >= num_items:\n",
        "                products = products[:num_items]\n",
        "                break\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logging.error(f\"Terjadi kesalahan saat mengirim permintaan: {e}\")\n",
        "            st.error(f\"Terjadi kesalahan saat mengirim permintaan: {e}\")\n",
        "            break\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            logging.error(f\"HTTP Error: {e}\")\n",
        "            st.error(f\"HTTP Error: {e}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Terjadi kesalahan yang tidak diketahui: {e}\")\n",
        "            st.error(f\"Terjadi kesalahan yang tidak diketahui: {e}\")\n",
        "            break\n",
        "        except WebDriverException as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "            break\n",
        "        finally:\n",
        "            if driver:\n",
        "                driver.quit()\n",
        "        page += 1\n",
        "    return products\n",
        "#---------------------------------------------------User Interface----------------------------------------------------------------------\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Scraping E-Commerce\")\n",
        "\n",
        "with st.expander(\"Settings :\"):\n",
        "    # Pilihan untuk memilih situs web\n",
        "    selected_site = st.selectbox(\"Pilih Situs Web :\", [\"klikindomaret.com\", \"tokopedia.com\", \"tokopedia.com(selenium)\"])\n",
        "\n",
        "    nama_barang = st.text_input(\"Masukkan Nama Barang :\")\n",
        "    num_items = st.number_input(\"Masukkan Estimasi Banyak Data :\", min_value = 1, step = 1, placeholder=\"Type a number...\")\n",
        "\n",
        "    download_format = st.selectbox(\"Pilih Format Unduhan :\", [\"XLSX\", \"CSV\", \"JSON\"])\n",
        "    st.info('Tekan \"Mulai Scraping\" kembali jika tampilan menghilang ', icon=\"ℹ️\")\n",
        "\n",
        "# Variabel tersembunyi untuk menyimpan hasil scraping\n",
        "hidden_data = []\n",
        "\n",
        "scraping_done = False  # Tambahkan variabel ini\n",
        "\n",
        "if selected_site == \"klikindomaret.com\":\n",
        "    if st.button(\"Mulai Scraping\"):\n",
        "        if not nama_barang:\n",
        "            st.error(\"Mohon isi Nama Barang.\")\n",
        "        else:\n",
        "            scraped_products = scrape_klikindomaret(nama_barang, num_items)\n",
        "            hidden_data = scraped_products  # Simpan data ke dalam variabel tersembunyi\n",
        "            scraping_done = True  # Set scraping_done menjadi True\n",
        "\n",
        "if selected_site ==\"tokopedia.com\":\n",
        "    st.warning(\"Jika mengalami error karena sedang dalam pengembangan. Silahkan pilih situs yang lain\", icon=\"⚠️\")\n",
        "    if st.button(\"Mulai Scraping\"):\n",
        "        if not nama_barang:\n",
        "            st.error(\"Mohon isi Nama Barang.\")\n",
        "        else:\n",
        "            scraped_products = scrape_tokped(nama_barang, num_items)\n",
        "            hidden_data = scraped_products  # Simpan data ke dalam variabel tersembunyi\n",
        "            scraping_done = True  # Set scraping_done menjadi True\n",
        "\n",
        "if selected_site == \"tokopedia.com(selenium)\":\n",
        "    st.warning(\"Jika mengalami error karena sedang dalam pengembangan. Silahkan pilih situs yang lain\", icon=\"⚠️\")\n",
        "    if st.button(\"Mulai Scraping\"):\n",
        "        if not nama_barang:\n",
        "            st.error(\"Mohon isi Nama Barang.\")\n",
        "        else:\n",
        "            scraped_products = scrape_tokped_with_selenium(nama_barang, num_items)\n",
        "            hidden_data = scraped_products  # Simpan data ke dalam variabel tersembunyi\n",
        "            scraping_done = True  # Set scraping_done menjadi True\n",
        "\n",
        "# Simpan DataFrame ke dalam file\n",
        "output_file = f\"scraped_{selected_site}_{nama_barang}.xlsx\"\n",
        "output_file_csv = f\"scraped_{selected_site}_{nama_barang}.csv\"\n",
        "output_file_json = f\"scraped_{selected_site}_{nama_barang}.json\"\n",
        "\n",
        "\n",
        "#---------------------------------------------------Download File & Hasil Scraping----------------------------------------------------------------------\n",
        "\n",
        "# Tampilkan hasil scraping\n",
        "if scraping_done:\n",
        "    if hidden_data:\n",
        "        # Menampilkan hasil sentimen dalam kotak yang dapat diperluas\n",
        "        with st.expander(f\"Hasil Scraping {selected_site} :\"):\n",
        "            st.write(pd.DataFrame(scraped_products))\n",
        "        if download_format == \"XLSX\":\n",
        "            df = pd.DataFrame(scraped_products)\n",
        "            df.to_excel(output_file, index=False)\n",
        "            st.download_button(label=f\"Unduh XLSX ({len(hidden_data)} data)\", data=open(output_file, \"rb\").read(), key=\"xlsx_download\", file_name=output_file)\n",
        "        elif download_format == \"CSV\":\n",
        "            df = pd.DataFrame(scraped_products)\n",
        "            csv = df.to_csv(index=False)\n",
        "            st.download_button(label=f\"Unduh CSV ({len(hidden_data)} data)\", data=csv, key=\"csv_download\", file_name=output_file_csv)\n",
        "        elif download_format == \"JSON\":\n",
        "            json_data = pd.DataFrame(scraped_products).to_json(orient=\"records\")\n",
        "            st.download_button(label=f\"Unduh JSON ({len(hidden_data)} data)\", data=json_data, key=\"json_download\", file_name=output_file_json)\n",
        "    elif not hidden_data:\n",
        "        st.warning(f\"Tidak ada data pada query '{nama_barang}'\", icon=\"⚠️\")\n",
        "\n",
        "if not scraping_done:\n",
        "    st.write(\"Tidak ada data untuk diunduh.\")\n",
        "\n",
        "st.divider()\n",
        "github_link = \"https://github.com/naufalnashif/\"\n",
        "st.markdown(f\"GitHub: [{github_link}]({github_link})\")\n",
        "instagram_link = \"https://www.instagram.com/naufal.nashif/\"\n",
        "st.markdown(f\"Instagram: [{instagram_link}]({instagram_link})\")\n",
        "st.write('Terima kasih telah mencoba demo ini!')"
      ],
      "metadata": {
        "id": "9za7PM0sUlse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping News Headline with Streamlit Framework (Cnbc, Detik, Tempo, )"
      ],
      "metadata": {
        "id": "s1uA-_SdVSAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------Requirements----------------------------------------------------------------------\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import date\n",
        "import time\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "#---------------------------------------------------Scraping Function----------------------------------------------------------------------\n",
        "\n",
        "@st.cache_data\n",
        "def scrape_cnbc_data(query, date, jumlah, param_kosong):\n",
        "    data = []\n",
        "    page = 1\n",
        "    progress_text = \"Scraping in progress. Please wait.\"\n",
        "    my_bar = st.progress(len(data), text=progress_text)\n",
        "\n",
        "\n",
        "    while len (data) < jumlah :\n",
        "        try :\n",
        "\n",
        "            url = f\"https://www.cnbcindonesia.com/search?query={query}&p={page}&kanal=&tipe=artikel&date={date}\"\n",
        "            user_agents = [\n",
        "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
        "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134\",\n",
        "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.2 Safari/605.1.15\",\n",
        "            ]\n",
        "\n",
        "            # Mendapatkan User-Agent acak\n",
        "            random_user_agent = random.choice(user_agents)\n",
        "\n",
        "            # Menggunakan User-Agent dalam permintaan HTTP\n",
        "            headers = {\n",
        "                \"User-Agent\": random_user_agent,\n",
        "                \"Accept-Language\": \"en-US,en;q=0.5\"\n",
        "            }\n",
        "            timeout = 10\n",
        "            response = requests.get(url, headers=headers, timeout = timeout)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            articles = soup.find_all('article')\n",
        "\n",
        "            if not articles:\n",
        "                break\n",
        "\n",
        "            for article in articles:\n",
        "                title = article.find('h2').text.strip()\n",
        "                link = article.find('a')['href']\n",
        "                category = article.find('span', class_ = 'label').text.strip()\n",
        "                date_category = article.find('span', class_='date').text.strip()\n",
        "                text_parts = date_category.split(' - ')\n",
        "                date = text_parts[1].strip()\n",
        "\n",
        "                data.append({\n",
        "                    'category': category,\n",
        "                    'date': date,\n",
        "                    'judul-berita': title,\n",
        "                    'link-berita': link,\n",
        "                })\n",
        "            if len(data) > jumlah:\n",
        "                data = data[:jumlah]\n",
        "            break\n",
        "\n",
        "            prop = min(len(data) / jumlah, 1)\n",
        "            my_bar.progress(prop, text=progress_text)\n",
        "            page += 1\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    time.sleep(1)\n",
        "    my_bar.empty()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "def scrape_detik_news(query, date, jumlah, param_kosong):\n",
        "    start_page = 1\n",
        "    base_url = \"https://www.detik.com/search/searchall\"\n",
        "    data = []\n",
        "    progress_text = \"Scraping in progress... Please wait...\"\n",
        "    my_bar = st.progress(len(data), text=progress_text)\n",
        "    timeout = 10\n",
        "\n",
        "    while len(data) < jumlah:\n",
        "        try:\n",
        "            params = {\n",
        "                \"query\": query,\n",
        "                \"siteid\": 2,\n",
        "                \"sortby\": \"time\",\n",
        "                \"page\": start_page\n",
        "            }\n",
        "\n",
        "            url = f'https://www.detik.com/search/searchall?query={query}&siteid=2&sortby=time&page={start_page}'\n",
        "            # Daftar beberapa User-Agent\n",
        "            user_agents = [\n",
        "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
        "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134\",\n",
        "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.2 Safari/605.1.15\",\n",
        "            ]\n",
        "\n",
        "            # Mendapatkan User-Agent acak\n",
        "            random_user_agent = random.choice(user_agents)\n",
        "\n",
        "            # Menggunakan User-Agent dalam permintaan HTTP\n",
        "            headers = {\n",
        "                \"User-Agent\": random_user_agent,\n",
        "                \"Accept-Language\": \"en-US,en;q=0.5\"\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout = timeout)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            articles = soup.find_all('article')\n",
        "\n",
        "            if not articles :\n",
        "                break\n",
        "            for article in articles :\n",
        "                title = article.find('h2').text.strip()\n",
        "                link = article.find('a')['href']\n",
        "                category = article.find('span', class_='category').text\n",
        "                date_category = article.find('span', class_='date').text\n",
        "                date = date_category.replace(category, '').strip()\n",
        "                data.append({\n",
        "                    'category': category,\n",
        "                    'date': date,\n",
        "                    'judul-berita': title,\n",
        "                    'link-berita': link,\n",
        "                })\n",
        "\n",
        "                if len(data) >= jumlah:\n",
        "                    data = data[:jumlah]\n",
        "                    break\n",
        "\n",
        "            prop = min(len(data) / jumlah, 1)\n",
        "            my_bar.progress(prop, text=progress_text)\n",
        "\n",
        "            start_page += 1\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "            break\n",
        "\n",
        "    time.sleep(1)\n",
        "    my_bar.empty()\n",
        "    return data\n",
        "\n",
        "@st.cache_data\n",
        "def scrape_viva_data(query, date, jumlah, param_kosong):\n",
        "    data = []\n",
        "    page = 1\n",
        "    progress_text = \"Scraping in progress. Please wait.\"\n",
        "    my_bar = st.progress(len(data), text=progress_text)\n",
        "\n",
        "\n",
        "    while len (data) < jumlah :\n",
        "        try :\n",
        "\n",
        "            url = f\"https://www.viva.co.id/search?q={query}\"\n",
        "\n",
        "            user_agents = [\n",
        "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
        "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134\",\n",
        "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.2 Safari/605.1.15\",\n",
        "            ]\n",
        "\n",
        "            # Mendapatkan User-Agent acak\n",
        "            random_user_agent = random.choice(user_agents)\n",
        "\n",
        "            # Menggunakan User-Agent dalam permintaan HTTP\n",
        "            headers = {\n",
        "                \"User-Agent\": random_user_agent,\n",
        "                \"Accept-Language\": \"en-US,en;q=0.5\"\n",
        "            }\n",
        "            timeout = 10\n",
        "            response = requests.get(url, headers=headers, timeout = timeout)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            articles = soup.find_all('div', class_='card-box ft240 margin-bottom-sm')\n",
        "            if not articles :\n",
        "                break\n",
        "\n",
        "            for article in articles :\n",
        "\n",
        "                title = article.find('h2', class_='title').text\n",
        "                link = article.find('a')['href']\n",
        "                category_element = article.find('span', class_=\"kanal cl-dark\")\n",
        "                category = category_element.text.strip() if category_element else None\n",
        "                date_element = article.find('h4', class_=\"date\")\n",
        "                date_before = date_element.text.strip() if date_element else None\n",
        "                date = date_before.replace(category, '')\n",
        "                data.append({\n",
        "                    'category': category,\n",
        "                    'date': date,\n",
        "                    'judul-berita': title,\n",
        "                    'link-berita': link,\n",
        "                })\n",
        "            if len(data) > jumlah:\n",
        "                data = data[:jumlah]\n",
        "            break\n",
        "\n",
        "            prop = min(len(data) / jumlah, 1)\n",
        "            my_bar.progress(prop, text=progress_text)\n",
        "            page += 1\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    time.sleep(1)\n",
        "    my_bar.empty()\n",
        "\n",
        "    return data\n",
        "\n",
        "@st.cache_data\n",
        "def scrape_tempo_data(query, date, jumlah, selected_channel):\n",
        "    data = []\n",
        "    domain = 1\n",
        "    max_domains = 5\n",
        "    progress_text = \"Scraping in progress. Please wait.\"\n",
        "    my_bar = st.progress(len(data), text=progress_text)\n",
        "    # List of channel values\n",
        "    default_channels = {\n",
        "        'All(Latest Only)': '',\n",
        "        'Nasional': '20',\n",
        "        'Metro': '19',\n",
        "        'Dunia': '5',\n",
        "        'Bisnis': '1',\n",
        "        'Bola': '21',\n",
        "        'Sport': '33',\n",
        "        'Gaya': '9',\n",
        "        'Seleb': '32',\n",
        "        'Cantik': '2',\n",
        "        'Tekno': '34',\n",
        "        'Otomotif': '23',\n",
        "        'Travel': '35',\n",
        "        'Blog': '43',\n",
        "        'Difabel': '44',\n",
        "        'Ramadan': '30',\n",
        "        'Kolom': '14',\n",
        "        'Fokus': '8',\n",
        "        'Creative Lab': '47',\n",
        "        'Event': '62',\n",
        "        'Data': '65',\n",
        "        'Cek Fakta': '66',\n",
        "        'Newsletter': '63',\n",
        "        'Inforial': '12'\n",
        "    }\n",
        "\n",
        "    # Ubah channels sesuai dengan selected_channel\n",
        "    if selected_channel != 'Defaults' and selected_channel in default_channels:\n",
        "        channels = {selected_channel: default_channels[selected_channel]}\n",
        "    else:\n",
        "        channels = default_channels\n",
        "    seen_titles = set()  # Set untuk melacak judul berita yang sudah muncul\n",
        "\n",
        "    try:\n",
        "        while len(data) < jumlah and domain <= max_domains:\n",
        "            for kanal, value in channels.items():\n",
        "                url = f\"https://www.tempo.co/search?waktu={waktu}&kanal={value}&subkanal=&domain={domain}&q={query}\"\n",
        "                user_agents = [\n",
        "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
        "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134\",\n",
        "                    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.2 Safari/605.1.15\",\n",
        "                ]\n",
        "                # Get a random User-Agent\n",
        "                random_user_agent = random.choice(user_agents)\n",
        "                # Use User-Agent in the HTTP request\n",
        "                headers = {\n",
        "                    \"User-Agent\": random_user_agent,\n",
        "                    \"Accept-Language\": \"en-US,en;q=0.5\"\n",
        "                }\n",
        "                timeout = 10\n",
        "                response = requests.get(url, headers=headers, timeout=timeout)\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                articles = soup.find_all('div', class_='card-box ft240 margin-bottom-sm')\n",
        "                if not articles:\n",
        "                    break\n",
        "                for article in articles:\n",
        "                    title = article.find('h2', class_='title').text\n",
        "                    # Hanya proses artikel yang belum pernah ditemui\n",
        "                    if title not in seen_titles:\n",
        "                        link = article.find('a')['href']\n",
        "                        category_element = article.find('span', class_=\"kanal cl-dark\")\n",
        "                        category = category_element.text.strip() if category_element else None\n",
        "                        date_element = article.find('h4', class_=\"date\")\n",
        "                        date_before = date_element.text.strip() if date_element else None\n",
        "                        date = date_before.replace(category, '')\n",
        "                        data.append({\n",
        "                            'category': category,\n",
        "                            'kanal' : kanal,\n",
        "                            'date': date,\n",
        "                            'judul-berita': title,\n",
        "                            'link-berita': link,\n",
        "                        })\n",
        "                        seen_titles.add(title)  # Tambahkan judul berita ke set\n",
        "                        if len(data) >= jumlah:\n",
        "                            break\n",
        "                if len(data) >= jumlah:\n",
        "                    break\n",
        "                prop = min(len(data) / jumlah, 1)\n",
        "                my_bar.progress(prop, text=progress_text)\n",
        "            domain += 1\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        st.error(f\"An error occurred: {e}\")\n",
        "    time.sleep(1)\n",
        "    my_bar.empty()\n",
        "    return data\n",
        "#---------------------------------------------------Data Cleaning (RegEx)----------------------------------------------------------------------\n",
        "\n",
        "def clean_text(text):\n",
        "    # Pastikan text adalah string\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    # Tahap-1: Menghapus karakter non-ASCII\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "    # Tahap-2: Menghapus URL\n",
        "    text = re.sub(r'http[s]?://.[a-zA-Z0-9./_?=%&#+!]+', '', text)\n",
        "    text = re.sub(r'pic.twitter.com?.[a-zA-Z0-9./_?=%&#+!]+', '', text)\n",
        "\n",
        "    # Tahap-3: Menghapus mentions\n",
        "    text = re.sub(r'@[\\w]+', '', text)\n",
        "\n",
        "    # Tahap-4: Menghapus hashtag\n",
        "    text = re.sub(r'#([\\w]+)', '', text)\n",
        "\n",
        "    # Tahap-5 Menghapus 'amp' yang menempel pada '&' dan 'gt' yang menempel pada '&'\n",
        "    text = re.sub(r'&amp;|&gt;', '', text)\n",
        "\n",
        "    # Tahap-6: Menghapus karakter khusus (simbol)\n",
        "    text = re.sub(r'[!$%^&*@#()_+|~=`{}\\[\\]%\\-:\";\\'<>?,./]', '', text)\n",
        "\n",
        "    # Tahap-7: Menghapus angka\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "\n",
        "    # Tahap-8: Menggabungkan spasi ganda menjadi satu spasi\n",
        "    text = re.sub(' +', ' ', text)\n",
        "\n",
        "    # Tahap-9: Menghapus spasi di awal dan akhir kalimat\n",
        "    text = text.strip()\n",
        "\n",
        "    # Tahap-10: Konversi teks ke huruf kecil\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tahap-11: koreksi duplikasi tiga karakter beruntun atau lebih (contoh. yukkk)\n",
        "    # text = re.sub(r'([a-zA-Z])\\1\\1', '\\\\1', text)\n",
        "    #text = re.sub(r'(.)(\\1{2,})', r'\\1\\1', text)\n",
        "    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "#---------------------------------------------------Normalisasi----------------------------------------------------------------------\n",
        "\n",
        "# Membaca kamus kata gaul Salsabila\n",
        "kamus_path = '_json_colloquial-indonesian-lexicon.txt'  # Ganti dengan path yang benar\n",
        "with open(kamus_path) as f:\n",
        "    data = f.read()\n",
        "lookp_dict = json.loads(data)\n",
        "\n",
        "# Dict kata gaul saya sendiri yang tidak masuk di dict Salsabila\n",
        "kamus_sendiri_path = 'kamus_gaul_custom.txt'\n",
        "with open(kamus_sendiri_path) as f:\n",
        "    kamus_sendiri = f.read()\n",
        "kamus_gaul_baru = json.loads(kamus_sendiri)\n",
        "\n",
        "# Menambahkan dict kata gaul baru ke kamus yang sudah ada\n",
        "lookp_dict.update(kamus_gaul_baru)\n",
        "\n",
        "# Fungsi untuk normalisasi kata gaul\n",
        "def normalize_slang(text, slang_dict):\n",
        "    words = text.split()\n",
        "    normalized_words = [slang_dict.get(word, word) for word in words]\n",
        "    return ' '.join(normalized_words)\n",
        "\n",
        "#---------------------------------------------------NLTK Remove Stopwords----------------------------------------------------------------------\n",
        "\n",
        "# Inisialisasi stopwords bahasa Indonesia\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"indonesian\"))\n",
        "\n",
        "def remove_stopwords(text, stop_words):\n",
        "    # Pecah teks menjadi kata-kata\n",
        "    words = text.split()\n",
        "\n",
        "    # Hapus stopwords bahasa Indonesia\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "def preprocessing_data(hidden_data):\n",
        "    # Initialize results\n",
        "    results_prep = []\n",
        "    df = pd.DataFrame(hidden_data)\n",
        "    texts = df[\"judul-berita\"]\n",
        "    # Process the text data\n",
        "    for text in texts:\n",
        "        cleaned_text = clean_text(text)\n",
        "        norm_slang_text = normalize_slang(cleaned_text, lookp_dict)\n",
        "        tanpa_stopwords = remove_stopwords(norm_slang_text, stop_words)\n",
        "\n",
        "        results_prep.append({\n",
        "            'judul-berita': text,\n",
        "            'cleaned-text' : cleaned_text,\n",
        "            'normalisasi-text' : norm_slang_text,\n",
        "            'stopwords-remove' : tanpa_stopwords,\n",
        "        })\n",
        "    return results_prep\n",
        "\n",
        "\n",
        "def eksplorasi_data(selected_options, results, colormap, words):\n",
        "    # Kolom pertama untuk Word Cloud\n",
        "    if 'Hasil EDA' in selected_options:\n",
        "        # Membagi tampilan menjadi dua kolom\n",
        "        columns = st.columns(2)\n",
        "        all_texts = \"\"\n",
        "        with columns[0]:\n",
        "            if results:\n",
        "                all_texts = all_texts = [result.get('stopwords-remove') for result in results if pd.notna(result.get('stopwords-remove'))]\n",
        "                all_texts = \" \".join(all_texts)\n",
        "\n",
        "                st.subheader(\"Word Cloud\")\n",
        "\n",
        "                if all_texts:\n",
        "                    wordcloud = WordCloud(width=800, height=500, background_color='white',\n",
        "                                            colormap=colormap,\n",
        "                                            contour_color='black',\n",
        "                                            contour_width=2,\n",
        "                                            mask=None).generate(all_texts)\n",
        "                    st.image(wordcloud.to_array())\n",
        "\n",
        "        # Kolom kedua untuk Most Common Words\n",
        "        with columns[1]:\n",
        "            st.subheader(\"Most Common Words\")\n",
        "\n",
        "            if all_texts:\n",
        "                word_counts = Counter(all_texts.split())\n",
        "                most_common_words = word_counts.most_common(words)\n",
        "\n",
        "                words, counts = zip(*most_common_words)\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                ax.bar(words, counts)\n",
        "                ax.set_xlabel(\"Kata-kata\")\n",
        "                ax.set_ylabel(\"Jumlah\")\n",
        "                ax.set_title(\"Kata-kata Paling Umum\")\n",
        "                ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "                st.pyplot(fig)\n",
        "@st.cache_data\n",
        "def scrape_and_explore_data(_scrape_function, query, date, jumlah, selected_options, colormap, words, param):\n",
        "    data_df = _scrape_function(query, date, jumlah, param)\n",
        "    hidden_data = data_df\n",
        "    scraping_done = True\n",
        "    results = preprocessing_data(hidden_data)\n",
        "\n",
        "    # Eksplorasi Data\n",
        "    eksplorasi_data(selected_options, results, colormap, words)\n",
        "    return hidden_data, scraping_done, results\n",
        "#---------------------------------------------------User Interface----------------------------------------------------------------------\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Aplikasi Web Scraping & Explorasi Data\")\n",
        "\n",
        "with st.expander(\"Scraping Settings :\"):\n",
        "    # Pilihan untuk memilih situs web\n",
        "    selected_site = st.selectbox(\"Pilih Situs Web :\", [\"CNBC Indonesia\", \"Detik.com\", \"Viva.co.id\", \"Tempo.co\", \"Liputan6.com\"])\n",
        "    if selected_site == \"Tempo.co\":\n",
        "        waktu = st.selectbox(\"Pilih Rentang Waktu :\", [\"1tahun\", \"1bulan\", \"1minggu\", \"1hari\", \"6jam\"])\n",
        "        selected_channel = st.selectbox(\"Pilih Kanal :\", ['Defaults','All(Latest Only)', 'Nasional', 'Metro', 'Dunia', 'Bisnis', 'Bola', 'Sport', 'Gaya', 'Seleb', 'Cantik', 'Tekno', 'Otomotif', 'Travel', 'Blog', 'Difabel', 'Ramadan', 'Kolom', 'Fokus', 'Creative Lab', 'Event', 'Data', 'Cek Fakta', 'Newsletter', 'Inforial'])\n",
        "    query = st.text_input(\"Masukkan Query :\").replace(' ', '+')\n",
        "\n",
        "    jumlah = st.number_input(\"Masukkan Estimasi Banyak Data :\", min_value = 1, step = 1, placeholder=\"Type a number...\")\n",
        "    date = date.today()\n",
        "    download_format = st.selectbox(\"Pilih Format Unduhan :\", [\"XLSX\", \"CSV\", \"JSON\", \"TXT\"])\n",
        "param_kosong = []\n",
        "with st.expander(\"Preference Settings :\"):\n",
        "    selected_options = st.multiselect(\n",
        "        'Pilih tampilan:',\n",
        "        ['Hasil Scraping', 'Hasil Preprocessing', 'Hasil EDA'],\n",
        "        [\"Hasil Scraping\", \"Hasil EDA\"]\n",
        "    )\n",
        "    if \"Hasil EDA\" in selected_options:\n",
        "        colormap = st.selectbox(\"Pilih Warna Wordclouds :\", [\"Greys\", \"Purples\", \"Blues\", \"Greens\", \"Oranges\", \"Reds\", \"YlOrBr\", \"YlOrRd\", \"OrRd\", \"PuRd\", \"RdPu\", \"BuPu\", \"GnBu\", \"PuBu\", \"YlGnBu\", \"PuBuGn\", \"BuGn\", \"YlGn\"])\n",
        "        words = st.number_input(\"Masukkan Jumlah Most Common Words :\", min_value = 1, max_value = 15, step = 1, value = 10, placeholder=\"Type a number...\")\n",
        "    else :\n",
        "        colormap = \"Greys\"\n",
        "        words = 10\n",
        "\n",
        "st.info('Tekan \"Mulai Scraping\" kembali jika tampilan menghilang ', icon=\"ℹ️\")\n",
        "\n",
        "#------------------------------------------------------------Bakcend----------------------------------------------------------------------------------\n",
        "\n",
        "# Variabel tersembunyi untuk menyimpan hasil scraping\n",
        "hidden_data = []\n",
        "\n",
        "scraping_done = False  # Tambahkan variabel ini\n",
        "\n",
        "if st.button(\"Mulai Scraping\"):\n",
        "    if not query:\n",
        "        st.error(\"Mohon isi query.\")\n",
        "    else:\n",
        "        # CNBC Indonesia\n",
        "        if selected_site == \"CNBC Indonesia\":\n",
        "            hidden_data, scraping_done, results = scrape_and_explore_data(scrape_cnbc_data, query, date.strftime(\"%Y/%m/%d\"), jumlah, selected_options, colormap, words, param_kosong)\n",
        "\n",
        "        # Detik.com\n",
        "        elif selected_site == \"Detik.com\":\n",
        "            hidden_data, scraping_done, results = scrape_and_explore_data(scrape_detik_news, query, date, jumlah, selected_options, colormap, words, param_kosong)\n",
        "\n",
        "        # Viva.co.id\n",
        "        elif selected_site == \"Viva.co.id\":\n",
        "            st.warning(\"Masih dalam penegmbangan, silahkan gunakan situs yang lain.\")\n",
        "            hidden_data, scraping_done, results = scrape_and_explore_data(scrape_viva_data, query, date, jumlah, selected_options, colormap, words, param_kosong)\n",
        "\n",
        "        # Tempo.co\n",
        "        elif selected_site == \"Tempo.co\":\n",
        "            st.warning(\"Masih dalam penegmbangan, silahkan gunakan situs yang lain.\")\n",
        "            hidden_data, scraping_done, results = scrape_and_explore_data(scrape_tempo_data, query, waktu, jumlah, selected_options, colormap, words, selected_channel)\n",
        "\n",
        "        # Liputan6.com\n",
        "        elif selected_site == \"Liputan6.com\":\n",
        "            st.error(\"Belum bisa dipakai.\")\n",
        "\n",
        "#---------------------------------------------------Download File & Hasil Scraping----------------------------------------------------------------------\n",
        "\n",
        "# Tampilkan hasil scraping\n",
        "if scraping_done:\n",
        "    if hidden_data:\n",
        "        df = pd.DataFrame(hidden_data)\n",
        "        df_prep = pd.DataFrame(results)\n",
        "        # Menampilkan hasil sentimen dalam kotak yang dapat diperluas\n",
        "        if 'Hasil Scraping' in selected_options:\n",
        "            with st.expander(f\"Hasil Scraping {selected_site} :\"):\n",
        "                st.write(df)\n",
        "        if 'Hasil Preprocessing' in selected_options:\n",
        "            with st.expander(f\"Hasil Preprocessing Data :\"):\n",
        "                st.write(df_prep)\n",
        "        if download_format == \"XLSX\":\n",
        "            df.to_excel(f\"hasil_scraping_{query}.xlsx\", index=False)\n",
        "            df_prep.to_excel(f\"hasil_preprocess_{query}.xlsx\", index=False)\n",
        "            st.download_button(label=f\"Unduh Hasil Scraping XLSX ({len(hidden_data)} data)\", data=open(f\"hasil_scraping_{query}.xlsx\", \"rb\").read(), key=\"xlsx_download\", file_name=f\"hasil_scraping_{query}.xlsx\")\n",
        "            st.download_button(label=f\"Unduh Hasil Preprocess XLSX ({len(results)} data)\", data=open(f\"hasil_preprocess_{query}.xlsx\", \"rb\").read(), key=\"xlsx_download_2\", file_name=f\"hasil_preprocess_{query}.xlsx\")\n",
        "        elif download_format == \"CSV\":\n",
        "            csv = df.to_csv(index=False)\n",
        "            csv_prep = df_prep.to_csv(index = False)\n",
        "            st.download_button(label=f\"Unduh Hasil Scraping CSV ({len(hidden_data)} data)\", data=csv, key=\"csv_download\", file_name=f\"hasil_scraping_{query}.csv\")\n",
        "            st.download_button(label=f\"Unduh Hasil Preprocess CSV ({len(results)} data)\", data=csv_prep, key=\"csv_download_2\", file_name=f\"hasil_preprocess_{query}.csv\")\n",
        "        elif download_format == \"JSON\":\n",
        "            json_data = pd.DataFrame(hidden_data, columns=[\"date\", \"judul-berita\", \"link-berita\"]).to_json(orient=\"records\")\n",
        "            json_data_prep = pd.DataFrame(results, columns=[\"Teks\", \"Cleaned Text\", \"Norm Text\", \"Tanpa Stopwords\"]).to_json(orient=\"records\")\n",
        "            st.download_button(label=f\"Unduh Hasil Scraping JSON ({len(hidden_data)} data)\", data=json_data, key=\"json_download\", file_name=f\"hasil_scraping_{query}.json\")\n",
        "            st.download_button(label=f\"Unduh Hasil Preprocess JSON ({len(results)} data)\", data=json_data_prep, key=\"json_download_2\", file_name=f\"hasil_preprocess_{query}.json\")\n",
        "        elif download_format == \"TXT\":\n",
        "            text_data = \"\\n\".join([f\"{row['date']} - {row['judul-berita']} - {row['link-berita']}\" for row in hidden_data])\n",
        "\n",
        "            st.download_button(label=f\"Unduh Hasil Scraping TXT ({len(hidden_data)} data)\", data=text_data, key=\"txt_download\", file_name=f\"hasil_scraping_{query}.txt\")\n",
        "\n",
        "    if not hidden_data:\n",
        "        st.warning(f\"Tidak ada data pada query '{query}'\", icon=\"⚠️\")\n",
        "if not scraping_done:\n",
        "    st.write(\"Tidak ada data untuk diunduh.\")\n",
        "\n",
        "st.divider()\n",
        "github_link = \"https://github.com/naufalnashif/\"\n",
        "st.markdown(f\"GitHub: [{github_link}]({github_link})\")\n",
        "instagram_link = \"https://www.instagram.com/naufal.nashif/\"\n",
        "st.markdown(f\"Instagram: [{instagram_link}]({instagram_link})\")\n",
        "st.write('Terima kasih telah mencoba demo ini!')"
      ],
      "metadata": {
        "id": "zXWyhtRIVVIX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}